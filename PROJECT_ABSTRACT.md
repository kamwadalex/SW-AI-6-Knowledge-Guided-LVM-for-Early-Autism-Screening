# Knowledge-Guided Large Vision Models for Early Autism Screening in Resource-Constrained Settings

## Abstract

This project develops a novel knowledge-driven framework for early autism screening by integrating a systematically constructed corpus of published diagnostic literature with Large Vision Models (LVMs) optimized for live or uploaded video analysis in resource-constrained settings. The corpus, derived from peer-reviewed studies, clinical guidelines, and culturally adapted assessment protocols, is annotated to capture visual and behavioral markers of autism such as:

- Gaze fixation, Facial affect recognition, Repetitive motor patterns
- Joint attention
- Atypical social reciprocity

This knowledge base is then aligned with LVMs through fine-tuning and symbolic constraint encoding, enabling the models not only to detect and classify autism-related behaviors but also to justify predictions in terms of established diagnostic evidence.

A core methodological contribution lies in the design of a multimodal pipeline that combines:

- Time-series video segmentation
- Symbolic reasoning layers
- Lightweight inference modules deployable on mobile and edge devices

This approach overcomes infrastructural constraints and outputs interpretable screening reports that link observed behaviors with literature-based diagnostic categories, thereby enhancing transparency, clinical validity, and trust in AI-assisted screening.

By bridging knowledge-guided corpora with scalable vision AI, this work pioneers an explainable, culturally responsive, and resource-efficient approach to autism diagnostics that directly addresses inequities in access to early detection services in low-resource environments.

## Keywords

- Autism Screening
- Large Vision Models
- Knowledge-Guided AI
- Resource-Constrained Health Settings
- Multimodal Diagnostics
- Corpus Development for Clinical AI